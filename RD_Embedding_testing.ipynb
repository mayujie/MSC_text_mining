{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RD_Embedding_testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOIIg8toKUK4H19lOS6Djcv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mayujie/MSC_text_mining/blob/master/RD_Embedding_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzVkJvUA8_hk"
      },
      "source": [
        "# [Amazon product data](http://jmcauley.ucsd.edu/data/amazon/index_2014.html)\n",
        "# [Amazon product data Julian McAuley, UCSD](http://jmcauley.ucsd.edu/data/amazon/)\n",
        "Files\n",
        "\n",
        "\"Small\" subsets for experimentation\n",
        "\n",
        "https://zhuanlan.zhihu.com/p/137537499\n",
        "\n",
        "http://baijiahao.baidu.com/s?id=1665370803562402243\n",
        "\n",
        "http://www.xiaomaidong.com/?p=1249\n",
        "\n",
        "https://towardsdatascience.com/double-your-google-colab-ram-in-10-seconds-using-these-10-characters-efa636e646ff\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fMvC2Ho82ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1645bb4f-4ef9-4ae7-c0c9-d028f61181ff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL3Ow4cS9BsV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f133174c-475f-447d-c2a4-36b1352f5265"
      },
      "source": [
        "root_path = '/content/drive/My Drive/Colab/MSC_RD'\n",
        "root_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab/MSC_RD'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXYyIif59Euj"
      },
      "source": [
        "## **Read from cleaned text file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMyCteQr9K9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ec0c72e-92d2-4f9f-a8c6-c75d3306813b"
      },
      "source": [
        "### download cleaned text data\n",
        "# !gdown https://drive.google.com/uc?id=1Kb8ApeUYUDMOO39utV-dQ00PZ0s8yOAb \n",
        "# !unzip cleaned_reviews_text.zip\n",
        "!unzip /content/drive/MyDrive/Colab/MSC_RD/cleaned_reviews_text.zip\n",
        "!rm /content/cleaned_reviews_text.zip\n",
        "!ls -lh *.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab/MSC_RD/cleaned_reviews_text.zip\n",
            "  inflating: cleaned_reviews_text.txt  \n",
            "rm: cannot remove '/content/cleaned_reviews_text.zip': No such file or directory\n",
            "-rw-r--r-- 1 root root 241M Nov 13 13:20 cleaned_reviews_text.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ae5lOS_9MKy"
      },
      "source": [
        "cleaned_text_ls = []\n",
        "with open('/content/cleaned_reviews_text.txt', 'r') as file:\n",
        "  for line in file:\n",
        "    cleaned_text_ls.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90r1-FB89NSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7844b6a-0746-46ab-d311-995cb82ecced"
      },
      "source": [
        "print(len(cleaned_text_ls))\n",
        "cleaned_text_ls[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "484826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['big expectations love english tv <COMMA> particular investigative detective stuff guy really boring <PERIOD> not appeal <PERIOD>\\n',\n",
              " 'highly recommend series <PERIOD> must anyone yearning watch grown television <PERIOD> complex characters plots keep one totally involved <PERIOD> thank amazin prime <PERIOD>\\n',\n",
              " 'one real snoozer <PERIOD> not believe anything read hear <COMMA> awful <PERIOD> no idea title means <PERIOD> neither <PERIOD>\\n',\n",
              " 'mysteries interesting <PERIOD> tension robson tall blond good not always believable <PERIOD> often seemed uncomfortable <PERIOD>\\n',\n",
              " 'show always excellent <COMMA> far british crime mystery one best ever made <PERIOD> stories well done acting top notch interesting twists realistic brutal storylines <PERIOD> show pulls no punches enters twisted minds criminals profiler psychiatrist helps northern english city police force <PERIOD> show looks like shot manchester called another name show <PERIOD> one episode not disc excellent prayer bone seperate disc <PERIOD> still crime shows not get much better one either side ocean <PERIOD> just great show never less well made episode <PERIOD> unfortunately like british shows get five shows year <COMMA> hour half shows <COMMA> still one could hope least year <PERIOD> realism depth main character tony hill excellent robson green well worth viewing just makes role truly part everyway <PERIOD> bet went crime scenes even real life research role <PERIOD> writers must applauded way average stories <PERIOD> let us hope show continues many years come <PERIOD>\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NufOlj_9QFl"
      },
      "source": [
        "## remove newline '\\n' from each row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upkxGUq6r35r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a22412d-64b3-4e41-b648-c95dc7aac47a"
      },
      "source": [
        "cleaned_text_ls = map(lambda s: s.strip('\\n'), cleaned_text_ls)\n",
        "cleaned_text_ls = list(cleaned_text_ls)\n",
        "cleaned_text_ls[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['big expectations love english tv <COMMA> particular investigative detective stuff guy really boring <PERIOD> not appeal <PERIOD>',\n",
              " 'highly recommend series <PERIOD> must anyone yearning watch grown television <PERIOD> complex characters plots keep one totally involved <PERIOD> thank amazin prime <PERIOD>',\n",
              " 'one real snoozer <PERIOD> not believe anything read hear <COMMA> awful <PERIOD> no idea title means <PERIOD> neither <PERIOD>',\n",
              " 'mysteries interesting <PERIOD> tension robson tall blond good not always believable <PERIOD> often seemed uncomfortable <PERIOD>',\n",
              " 'show always excellent <COMMA> far british crime mystery one best ever made <PERIOD> stories well done acting top notch interesting twists realistic brutal storylines <PERIOD> show pulls no punches enters twisted minds criminals profiler psychiatrist helps northern english city police force <PERIOD> show looks like shot manchester called another name show <PERIOD> one episode not disc excellent prayer bone seperate disc <PERIOD> still crime shows not get much better one either side ocean <PERIOD> just great show never less well made episode <PERIOD> unfortunately like british shows get five shows year <COMMA> hour half shows <COMMA> still one could hope least year <PERIOD> realism depth main character tony hill excellent robson green well worth viewing just makes role truly part everyway <PERIOD> bet went crime scenes even real life research role <PERIOD> writers must applauded way average stories <PERIOD> let us hope show continues many years come <PERIOD>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dIwOqVwsRPg"
      },
      "source": [
        "# ## read data into dataframe\n",
        "# import pandas as pd\n",
        "# df_review = pd.DataFrame(cleaned_text_ls) \n",
        "# df_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oG7nOGir78k"
      },
      "source": [
        "# cleaned_text_ls[9568]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMHPvsVxr996"
      },
      "source": [
        "# def find_noise(reg, datalist):\n",
        "#   count = 0\n",
        "#   found_list = []\n",
        "\n",
        "#   for i, row in enumerate(datalist):\n",
        "#     found = re.findall(reg, row)\n",
        "\n",
        "#     if found != []:\n",
        "#       count += 1\n",
        "#       # print(found[0])\n",
        "#       # found = list(filter(None, found[0]))\n",
        "#       # print(found)\n",
        "#       found_list.append([i ,found])\n",
        "  \n",
        "#   return count, found_list\n",
        "#   # return found_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrOicEgjsAIA"
      },
      "source": [
        "# import re\n",
        "# ## check tokens for each row\n",
        "# count_key, found_key = find_noise(r\"<\\w+>\", cleaned_text_ls) ## \"<\\w+>\" \" <UNK> \"\n",
        "# print(count_key, len(found_key))\n",
        "# found_key[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNQ9ZGB-6mEB"
      },
      "source": [
        "# ss = [i[0] for i in found_key]\n",
        "# print(ss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFCrqsvdsBqC"
      },
      "source": [
        "# ss = []\n",
        "# for i in found_key:\n",
        "#   for j in i[1]: \n",
        "#     ss.append(j)\n",
        "\n",
        "# from collections import Counter \n",
        "# print(len(Counter(ss).most_common()))\n",
        "# Counter(ss).most_common() # 12, 14 tokens with each amount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EETEKivJ9JO"
      },
      "source": [
        "**empty reviews: [(' \\<UNK\\> ', 138)]**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBKZX78YwY7Q"
      },
      "source": [
        "# reviews_text_merge = \" \".join(cleaned_text_ls)\n",
        "# print(len(reviews_text_merge)) ## 252078789\n",
        "# reviews_text_merge[:1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7mnmFFpz5cn"
      },
      "source": [
        "# with open(\"reviews_text_merge.txt\", \"w\") as file:\n",
        "#     file.write(\"%s\" % reviews_text_merge)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZifEZRUV1FP5"
      },
      "source": [
        "# # Compress review text merge into zip file\n",
        "# !zip \"{root_path}\"/reviews_text_merge.zip reviews_text_merge.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGq2KixI1k8-"
      },
      "source": [
        "# # https://drive.google.com/file/d/1Wrw1Wk2w6pkdfdQ3asOTX1_s_7djo1yk/view?usp=sharing\n",
        "# !unzip \"{root_path}\"/reviews_text_merge.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbKBV8Db2A9M"
      },
      "source": [
        "# with open('/content/reviews_text_merge.txt') as file:\n",
        "#     text = file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb_hkg212L20"
      },
      "source": [
        "# # words = text.split()\n",
        "# words = \" \".join(cleaned_text_ls).split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmL1pdStC31T"
      },
      "source": [
        "# from collections import Counter \n",
        "\n",
        "# word_counts = Counter(words)\n",
        "# print('Total number of words in cleaned corpus:', len(words))\n",
        "# print('Number of unique words in cleaned corpus:', len(Counter(words)), len(Counter(words).most_common()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02XQvfTm-Fyu"
      },
      "source": [
        "Total number of words in cleaned corpus: 36436399\n",
        "\n",
        "Number of unique words in cleaned corpus: 55984 55984"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QlL8qYvC6Xx"
      },
      "source": [
        "# word_counts.most_common()[44187-10:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sduHZ543LHh"
      },
      "source": [
        "# vocab_to_int, int_to_vocab = create_lookup_tables(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-ALO8Eb3QIj"
      },
      "source": [
        "# int_words = [vocab_to_int[word] for word in words]\n",
        "# print(words[:30])\n",
        "# print(int_words[:30])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGqGpHxG-Nkc"
      },
      "source": [
        "['big', 'expectations', 'love', 'english', 'tv', '<COMMA>', 'particular', 'investigative', 'detective', 'stuff', 'guy', 'really', 'boring', '<PERIOD>', 'not', 'appeal', '<PERIOD>', 'highly', 'recommend', 'series', '<PERIOD>', 'must', 'anyone', 'yearning', 'watch', 'grown', 'television', '<PERIOD>', 'complex', 'characters']\n",
        "\n",
        "[101, 1803, 20, 1865, 791, 1, 749, 19071, 4452, 169, 785, 16, 1481, 0, 2, 2068, 0, 198, 90, 318, 0, 226, 311, 8751, 409, 1978, 2657, 0, 1951, 577]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJkaqoCo_yvz"
      },
      "source": [
        "## create word_counts dictionary for all words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxNvtyW1LaWS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203a4e66-e356-423d-bd52-1ff942256100"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable          Type             Data/Info\n",
            "--------------------------------------------\n",
            "cleaned_text_ls   list             n=484826\n",
            "drive             module           <module 'google.colab.dri<...>s/google/colab/drive.py'>\n",
            "file              TextIOWrapper    <_io.TextIOWrapper name='<...>ode='r' encoding='UTF-8'>\n",
            "line              str              nice head lamp <EXCLAMATI<...>flashing modes <PERIOD>\\n\n",
            "root_path         str              /content/drive/My Drive/Colab/MSC_RD\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nS7YgGm_uye"
      },
      "source": [
        "words = []\n",
        "for row in cleaned_text_ls:\n",
        "  for i in row.split():\n",
        "    # print(i)\n",
        "    words.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyqmIZWL_4mP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67bbf1ac-4c6f-4363-a447-b943c5399d3b"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "print('Total number of words in cleaned corpus:', len(words))\n",
        "print('Number of unique words in cleaned corpus:', len(Counter(words)), len(Counter(words).most_common()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of words in cleaned corpus: 36436399\n",
            "Number of unique words in cleaned corpus: 55984 55984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IblzwWEJVbf"
      },
      "source": [
        "### **For 8 classes data (Current):**\n",
        "Length of reviews: **484826**\n",
        "\n",
        "Total number of words in cleaned corpus: **36436399**\n",
        "\n",
        "Number of unique words in cleaned corpus: **55984** **55984**\n",
        "\n",
        "### For 6 classes data:\n",
        "\n",
        "Length of reviews: **199096**\n",
        "\n",
        "Total number of words in cleaned corpus: 18241862\n",
        "\n",
        "Number of unique words in cleaned corpus: 44187 44187\n",
        "\n",
        "### For 10 classes data:\n",
        "\n",
        "Length of reviews: **2360871**\n",
        "\n",
        "Total number of words in cleaned corpus: 145318404\n",
        "\n",
        "Number of unique words in cleaned corpus: 110454 110454"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8RZoX4J_6sN"
      },
      "source": [
        "# Remove all words with  5 or fewer occurences\n",
        "from collections import Counter \n",
        "\n",
        "def create_lookup_tables(words):\n",
        "  word_counts = Counter(words)\n",
        "  # print(word_counts.most_common(10))\n",
        "  # print(word_counts.most_common())\n",
        "  ## sorting the words from most to least frequent in text occurrence, descending\n",
        "  sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "  # print(len(sorted_vocab)) # 110454\n",
        "  ## create int_to_vocab dictionaries\n",
        "  int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
        "  vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
        "\n",
        "  return vocab_to_int, int_to_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjNtnQ9oUVVu"
      },
      "source": [
        "[('\\<PERIOD\\>', 1671352),\n",
        " ('\\<COMMA\\>', 1266882),\n",
        " ('not', 341506),\n",
        " ('album', 140815),\n",
        " ('one', 140781),\n",
        " ('like', 127551),\n",
        " ('so', 120402),\n",
        " ('\\<EXCLAMATION\\>', 95972),\n",
        " ('just', 95320),\n",
        " ('good', 89656)]\n",
        "\n",
        "12 Tokens for 6 classes data\n",
        "\n",
        "[('\\<PERIOD\\>', 1671352),\n",
        " ('\\<COMMA\\>', 1266882),\n",
        " ('\\<EXCLAMATION\\>', 95972),\n",
        " ('\\<QUESTION\\>', 38148),\n",
        " ('\\<PRICE\\>', 9371),\n",
        " ('\\<HUNDREDPERCENTAGE\\>', 1098),\n",
        " ('\\<NOTCONTRACT\\>', 638),\n",
        " ('\\<MENTION\\>', 197),\n",
        " ('\\<AMAZONURL\\>', 185),\n",
        " ('\\<URL\\>', 136),\n",
        " ('\\<UNK\\>', 54),\n",
        " ('\\<YOUTUBEURL\\>', 8)]\n",
        "\n",
        " **14 Tokens for 8 classes data (Current)**\n",
        "\n",
        " [('\\<PERIOD\\>', 3476956),\n",
        " ('\\<COMMA\\>', 2399259),\n",
        " ('\\<EXCLAMATION\\>', 211162),\n",
        " ('\\<QUESTION\\>', 64627),\n",
        " ('\\<PRICE\\>', 29257),\n",
        " ('\\<HUNDREDPERCENTAGE\\>', 4389),\n",
        " ('\\<NOTCONTRACT\\>', 986),\n",
        " ('\\<AMAZONURL\\>', 506),\n",
        " ('\\<MENTION\\>', 288),\n",
        " ('\\<URL\\>', 246),\n",
        " ('\\<UNK\\>', 138),\n",
        " ('\\<YOUTUBEURL\\>', 25),\n",
        " ('\\<GOOGLEURL\\>', 13),\n",
        " ('\\<STAR\\>', 9)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Z_IiCiJIUL"
      },
      "source": [
        "## lookup table (word to int and vice versa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvQ0-Hr3jBBo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc36d0c2-d186-4385-c91b-6b0044c23407"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable               Type             Data/Info\n",
            "-------------------------------------------------\n",
            "Counter                type             <class 'collections.Counter'>\n",
            "cleaned_text_ls        list             n=484826\n",
            "create_lookup_tables   function         <function create_lookup_tables at 0x7fbc82550c80>\n",
            "drive                  module           <module 'google.colab.dri<...>s/google/colab/drive.py'>\n",
            "file                   TextIOWrapper    <_io.TextIOWrapper name='<...>ode='r' encoding='UTF-8'>\n",
            "i                      str              <PERIOD>\n",
            "line                   str              nice head lamp <EXCLAMATI<...>flashing modes <PERIOD>\\n\n",
            "root_path              str              /content/drive/My Drive/Colab/MSC_RD\n",
            "row                    str              nice head lamp <EXCLAMATI<...>> flashing modes <PERIOD>\n",
            "words                  list             n=36436399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJQlFJmsWMOr"
      },
      "source": [
        "vocab_to_int, int_to_vocab = create_lookup_tables(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7wHYvLIvbFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8873968e-9ecc-4165-8f70-b689a9da8dff"
      },
      "source": [
        "## words into int numbers\n",
        "int_words = [vocab_to_int[word] for word in words]\n",
        "\n",
        "print(len(int_words))\n",
        "print(words[:20])\n",
        "print(cleaned_text_ls[0])\n",
        "print(int_words[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36436399\n",
            "['big', 'expectations', 'love', 'english', 'tv', '<COMMA>', 'particular', 'investigative', 'detective', 'stuff', 'guy', 'really', 'boring', '<PERIOD>', 'not', 'appeal', '<PERIOD>', 'highly', 'recommend', 'series']\n",
            "big expectations love english tv <COMMA> particular investigative detective stuff guy really boring <PERIOD> not appeal <PERIOD>\n",
            "[101, 1803, 20, 1865, 791, 1, 749, 19071, 4452, 169, 785, 16, 1481, 0, 2, 2068, 0, 198, 90, 318]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn1mVLrdmXtw"
      },
      "source": [
        "## (just for check, optional)\n",
        "# int_words = []\n",
        "# for row in cleaned_text_ls:\n",
        "#   int_words.append([vocab_to_int[word] for word in row.split()])\n",
        "\n",
        "# print(len(int_words))\n",
        "# print(words[:20])\n",
        "# print(cleaned_text_ls[0])\n",
        "# print(int_words[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXbgRffpQYkZ"
      },
      "source": [
        "# cleaned_text_ls = [row.split() for row in cleaned_text_ls]\n",
        "# # print(cleaned_text_ls[:2])\n",
        "# len(cleaned_text_ls) #484826"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDPUYb2LKz1a"
      },
      "source": [
        "# ## convert each review to word tokens array\n",
        "# for i, row in enumerate(cleaned_text_ls):\n",
        "#   cleaned_text_ls[i] = [word for word in row.split()]\n",
        "\n",
        "# len(cleaned_text_ls) # 484826"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGnmpSGo75qT"
      },
      "source": [
        "# Subsampling (optional) \n",
        "I tried in this case, but seems not good. Because i've done stopwords removal before, and then I perform subsampling it leads to appearing a lot of review with lenght 0 or 1 word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cizzK3LPI5yG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56bd046-2d7c-4b39-ce15-4a3c644d7475"
      },
      "source": [
        "print('total number of word ', len(words))\n",
        "print(len(Counter(words)), len(Counter(int_words)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total number of word  36436399\n",
            "55984 55984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-Ny5zpIKnJW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21819925-a1f6-4f81-e54c-490cdc9ab108"
      },
      "source": [
        "print(cleaned_text_ls[0])\n",
        "print(words[:20])\n",
        "print(int_words[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "big expectations love english tv <COMMA> particular investigative detective stuff guy really boring <PERIOD> not appeal <PERIOD>\n",
            "['big', 'expectations', 'love', 'english', 'tv', '<COMMA>', 'particular', 'investigative', 'detective', 'stuff', 'guy', 'really', 'boring', '<PERIOD>', 'not', 'appeal', '<PERIOD>', 'highly', 'recommend', 'series']\n",
            "[101, 1803, 20, 1865, 791, 1, 749, 19071, 4452, 169, 785, 16, 1481, 0, 2, 2068, 0, 198, 90, 318]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56SJDWsx8pSz"
      },
      "source": [
        "# from collections import Counter\n",
        "# import random\n",
        "# import numpy as np\n",
        "\n",
        "# threshold = 1e-5\n",
        "# word_counts = Counter([vocab_to_int[word] for word in words])\n",
        "\n",
        "# # # print 1st key value pair in list\n",
        "# # print(list(word_counts.items())[0]) #  dictionary of int_words, how many times they appear\n",
        "# total_count = len([vocab_to_int[word] for word in words])\n",
        "# print('total_count number of word: ', total_count)\n",
        "\n",
        "# # calculate the frequency of occurrence for each word in our vocab\n",
        "# freqs = {word: count/total_count for word, count in word_counts.items()}\n",
        "# p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
        "\n",
        "# # discard some frequent words, according to the subsampling equation\n",
        "# # create a new list of words for training\n",
        "# # generate a random value between 0 and 1, then check if that value < 1 - drop_p for that word , i want to keep this word with a prob of 1 - drop_p\n",
        "# # if drop_p is 0.98, then keep word prob is 1 - 0.98 = 0.02, which will be 0.02. if generate a value < 0.02, then it will keep this word in list\n",
        "\n",
        "# # most of 0 and 1 are gone, subsampling for \"words\" variable\n",
        "# train_words = [word for word in [vocab_to_int[word] for word in words] if random.random() < (1 - p_drop[word])]\n",
        "\n",
        "# print(train_words[:30])\n",
        "# print('len of train word', len(train_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk7cFuePDQ5X"
      },
      "source": [
        "## method of subsampling for each row \n",
        "# train_words = []\n",
        "# for row in int_words:\n",
        "#   train_words.append([word for word in row if random.random() < (1 - p_drop[word])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpy0dYgCJszz"
      },
      "source": [
        "[20, 749, 19071, 4452, 785, 1481, 2068, 90, 318, 311, 8751, 409, 1978, 2657, 1951, 2300, 34267, 32079, 7315, 2702, 46542, 15474, 4064, 2423, 1728, 1224, 2996, 6078, 7786, 3518]\n",
        "\n",
        "len of train word 9193920"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjbrjfq7NiB0"
      },
      "source": [
        "# # len(cleaned_text_ls), len(train_words) # (484826, 484826)\n",
        "# idx = 20\n",
        "# print(cleaned_text_ls[0])\n",
        "# print(int_words[:idx])\n",
        "# print(words[:idx])\n",
        "# print(train_words[:idx])\n",
        "# print([int_to_vocab[i] for i in train_words[:idx]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbMzU9_8dCE-"
      },
      "source": [
        "['comedy', 'matter', 'taste', '<PERIOD>', 'guy', 'little', 'funny', 'others', 'far', 'funnier', 'guy']\n",
        "\n",
        "[2628, 578, 30, 0, 785, 21, 902, 241, 100, 12538, 785]\n",
        "\n",
        "[785, 12538, 785]\n",
        "\n",
        "['guy', 'funnier', 'guy']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBAeokO2Pc5f"
      },
      "source": [
        "# ls = []\n",
        "# for row in train_words:\n",
        "#   for i in row:\n",
        "#     ls.append(i)\n",
        "\n",
        "# len(ls) ## 9188684 words left after subsampling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjC_BWAGTsKX"
      },
      "source": [
        "# ls_word = [int_to_vocab[i] for i in ls] ## convert to word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm7OghfcYmaE"
      },
      "source": [
        "# [i for i,j in enumerate(train_words) if (len(j)==0)] ## 5607 empty row appears\n",
        "# [i for i,j in enumerate(train_words) if (len(j)==1)] ## 15105 rows with only one word\n",
        "# [i for i,j in enumerate(train_words) if (len(j)==2)] ## 24584 rows with only two word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7eR5kJRdbTk"
      },
      "source": [
        "# Making batches of data\n",
        "\n",
        "window size\n",
        "\n",
        "http://jalammar.github.io/illustrated-word2vec/#:~:text=The%20Gensim%20default%20window%20size,factor%20of%20the%20training%20process.\n",
        "\n",
        "https://stackoverflow.com/questions/22272370/word2vec-effect-of-window-size-used/30447723\n",
        "\n",
        "https://stackoverflow.com/questions/59872029/word2vec-window-size-at-sentence-boundaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRgqdKVydc_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3a475f-8664-490a-bc48-b082ede164fc"
      },
      "source": [
        "%whos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable               Type             Data/Info\n",
            "-------------------------------------------------\n",
            "Counter                type             <class 'collections.Counter'>\n",
            "cleaned_text_ls        list             n=484826\n",
            "create_lookup_tables   function         <function create_lookup_tables at 0x7fbc82550c80>\n",
            "drive                  module           <module 'google.colab.dri<...>s/google/colab/drive.py'>\n",
            "file                   TextIOWrapper    <_io.TextIOWrapper name='<...>ode='r' encoding='UTF-8'>\n",
            "i                      str              <PERIOD>\n",
            "int_to_vocab           dict             n=55984\n",
            "int_words              list             n=36436399\n",
            "line                   str              nice head lamp <EXCLAMATI<...>flashing modes <PERIOD>\\n\n",
            "root_path              str              /content/drive/My Drive/Colab/MSC_RD\n",
            "row                    str              nice head lamp <EXCLAMATI<...>> flashing modes <PERIOD>\n",
            "vocab_to_int           dict             n=55984\n",
            "words                  list             n=36436399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogeXbxDiZtIz"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_target(words, idx, window_size=5):\n",
        "  ''' Get a list of words in a window around an index. '''\n",
        "  \n",
        "  # R is random integer in the range of 1:C, R is the size\n",
        "  R = np.random.randint(1, window_size+1)\n",
        "  print(\"R: \", R, \" len of input sentence \", len(words))\n",
        "  \n",
        "  # define the start and stop indices of context window\n",
        "  # start will be a range of words in the past, index of word interest - R, only if it not gives negative idx\n",
        "  # if it gives a negative value, then just set idx to the start of list of words idx 0\n",
        "  start = idx - R if (idx - R) > 0 else 0\n",
        "  \n",
        "  # stop idx is where future words end\n",
        "  stop = idx + R\n",
        "  \n",
        "  # finally we don't want my target context to include the word at the past (word interest) in idx\n",
        "  target_words = words[start:idx] + words[idx+1:stop+1]\n",
        "\n",
        "  return list(target_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS_jDDdvcbBN",
        "outputId": "401d99e7-3ab1-433d-ad7f-b0d86ab48b06"
      },
      "source": [
        "idx = 6 # word index of interest\n",
        "print('Input: ', int_words[:10])\n",
        "\n",
        "target = get_target(int_words[:10], idx, window_size=5)\n",
        "print('Target: ', target)  # you should get some indices around the idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  [101, 1803, 20, 1865, 791, 1, 749, 19071, 4452, 169]\n",
            "R:  3  len of input sentence  10\n",
            "Target:  [1865, 791, 1, 19071, 4452, 169]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YqRaENe1J4d"
      },
      "source": [
        "# Generating Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaupgvUUPGiK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}