{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Basic methods of Natural Language Processing(NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, Lemmatization and Stop Word Removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       reviewerID        asin      reviewerName helpful  \\\n",
      "0  A30TL5EWN6DFXT  120401325X         christina  [0, 0]   \n",
      "1   ASY55RVNIL0UD  120401325X          emily l.  [0, 0]   \n",
      "2  A2TMXE2AFO7ONB  120401325X             Erica  [0, 0]   \n",
      "3   AWJ0WZQYMYFQ4  120401325X                JM  [4, 4]   \n",
      "4   ATX7CZYFXI1KW  120401325X  patrice m rogoza  [2, 3]   \n",
      "\n",
      "                                          reviewText  overall  \\\n",
      "0  They look good and stick good! I just don't li...      4.0   \n",
      "1  These stickers work like the review says they ...      5.0   \n",
      "2  These are awesome and make my phone look so st...      5.0   \n",
      "3  Item arrived in great time and was in perfect ...      4.0   \n",
      "4  awesome! stays on, and looks great. can be use...      5.0   \n",
      "\n",
      "                                     summary  unixReviewTime   reviewTime  \n",
      "0                                 Looks Good      1400630400  05 21, 2014  \n",
      "1                      Really great product.      1389657600  01 14, 2014  \n",
      "2                             LOVE LOVE LOVE      1403740800  06 26, 2014  \n",
      "3                                      Cute!      1382313600  10 21, 2013  \n",
      "4  leopard home button sticker for iphone 4s      1359849600   02 3, 2013  \n"
     ]
    }
   ],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('reviews_Cell_Phones_and_Accessories_5.json.gz')\n",
    "\n",
    "# take 5 head of data as example\n",
    "see = df.head(5)\n",
    "print(see)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They look good and stick good! I just don't li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These stickers work like the review says they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These are awesome and make my phone look so st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Item arrived in great time and was in perfect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome! stays on, and looks great. can be use...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText\n",
       "0  They look good and stick good! I just don't li...\n",
       "1  These stickers work like the review says they ...\n",
       "2  These are awesome and make my phone look so st...\n",
       "3  Item arrived in great time and was in perfect ...\n",
       "4  awesome! stays on, and looks great. can be use..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the the column \"reviewText\"\n",
    "see[[\"reviewText\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They look good and stick good! I just don't like the rounded shape because I was always bumping it and Siri kept popping up and it was irritating. I just won't buy a product like this again\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take row No.0 as example\n",
    "ls_txt = see[\"reviewText\"][0]\n",
    "ls_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text analysis has the following processes\n",
    "# 1: Tokenization\n",
    "# 2: Lemmatization\n",
    "# 3: Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'look', 'good', 'and', 'stick', 'good!', 'I', 'just', \"don't\", 'like', 'the', 'rounded', 'shape', 'because', 'I', 'was', 'always', 'bumping', 'it', 'and', 'Siri', 'kept', 'popping', 'up', 'and', 'it', 'was', 'irritating.', 'I', 'just', \"won't\", 'buy', 'a', 'product', 'like', 'this', 'again']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "# It is the process of breaking text into smaller chunks/tokens\n",
    "\n",
    "# call an instance of white space tokenizer\n",
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(ls_txt)\n",
    "print(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'look', 'good', 'and', 'stick', 'good', '!', 'I', 'just', 'do', \"n't\", 'like', 'the', 'rounded', 'shape', 'because', 'I', 'was', 'always', 'bumping', 'it', 'and', 'Siri', 'kept', 'popping', 'up', 'and', 'it', 'was', 'irritating.', 'I', 'just', 'wo', \"n't\", 'buy', 'a', 'product', 'like', 'this', 'again']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There can be instances where we need to tokenize using white space as well as punctuation. \n",
    "# This can be done using TreebankWord Tokenizer. Simply put, don't word will be broken into 'do' and 'n't'\n",
    "\n",
    "tokenize=nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens = tokenize.tokenize(ls_txt)\n",
    "print(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'look', 'good', 'and', 'stick', 'good', '!', 'I', 'just', 'don', \"'\", 't', 'like', 'the', 'rounded', 'shape', 'because', 'I', 'was', 'always', 'bumping', 'it', 'and', 'Siri', 'kept', 'popping', 'up', 'and', 'it', 'was', 'irritating', '.', 'I', 'just', 'won', \"'\", 't', 'buy', 'a', 'product', 'like', 'this', 'again']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#WorkPunct tokenizer creates , . ' and \" as individual Tokens\n",
    "\n",
    "tokenizer=nltk.tokenize.WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(ls_txt)\n",
    "print(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After breaking the text into individual words (tokens), we want to ensure that each of them are converted into their base form. \n",
    "# This base form is called lemma. This can be done using Lemmatization and Stemming. The key difference between them is mentioned below.\n",
    "\n",
    "# Stemming: It converts different forms of the word by chopping off the suffixes.It normally results in ambiguous words with no meanings.\n",
    "\n",
    "# Lemmatization: It converts different forms of the same word by considering the  morphological  context of the text. \n",
    "# For instance study and studying are treated in the same way and are converted to 'study'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They', 'look', 'good', 'and', 'stick', 'good', '!', 'I', 'just', 'do', \"n't\", 'like', 'the', 'rounded', 'shape', 'because', 'I', 'was', 'always', 'bumping', 'it', 'and', 'Siri', 'kept', 'popping', 'up', 'and', 'it', 'was', 'irritating.', 'I', 'just', 'wo', \"n't\", 'buy', 'a', 'product', 'like', 'this', 'again']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "# Next we want to ensure that tokens in the text are normalised\n",
    "# Meaning study and studying are treated in the same way\n",
    "# This can be achieved by using either stemming or lemmatization\n",
    "\n",
    "#Stemming:Chops off suffixes.Uses Porter Stemming method.Disadvantage is that is results in non-words\n",
    "#Lemmatization refers to doing things keeping usage and morphology in mind\n",
    "#It returns the base or dictionary form of the word which is known as lemma\n",
    "#For lemmatization we use WordNetLemmatizer found in NLTK library\n",
    "\n",
    "# Using 'Stemming' example\n",
    "tokenizer= nltk.tokenize.TreebankWordTokenizer()\n",
    "tokens=tokenizer.tokenize(ls_txt)\n",
    "print(tokens)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"they look good and stick good ! I just do n't like the round shape becaus I wa alway bump it and siri kept pop up and it wa irritating. I just wo n't buy a product like thi again\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\" \".join(stemmer.stem(i) for i in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the following things\n",
    "# rounded has been converted to round\n",
    "# bumping has been converted to bump\n",
    "# In general sses forms get converted to ss: Example caresses-->caress\n",
    "# ies --> i : Example studies-->studi\n",
    "# s-->singular form: Example cats-->cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They look good and stick good ! I just do n't like the round shape because I be always bump it and Siri keep pop up and it be irritating. I just wo n't buy a product like this again\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's use Lemmatizer and compare the outputs\n",
    "# nltk.download('wordnet')\n",
    "# print(ls_txt)\n",
    "lemma = nltk.stem.WordNetLemmatizer()\n",
    "\" \".join(lemma.lemmatize(i,'v') for i in tokens)\n",
    "# Here we can see that an extra argument in the form of 'v'(verb) has been provided\n",
    "# This is to explicitly specify the Part of Speech(POS)\n",
    "# If we dont specify the POS then WordNetLemmatizer assumes everything to be noun and hence \n",
    "# Inflectional various forms of the same root word will return different results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining the extra argument 'v' in WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run running ran'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not supplying POS\n",
    "\" \".join(lemma.lemmatize(i) for i in ['run','running','ran'])\n",
    "# As can be seen that it in absence of POS, it doesnt do lemmatization properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run run run'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supplying POS\n",
    "\" \".join(lemma.lemmatize(i,'v') for i in ['run','running','ran'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StopWord Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  After breaking and normalizing the text, we need to also ensure that there are no articles, determiners, prepositions etc. \n",
    "# In most situations it is necessary to remove them as they don't add any meaning  to the analysis. \n",
    "# The process of removing them is known as Stop word removal. \n",
    "# There is a custom list in python that contains list of words which can be used for cleaning text from any stop word. \n",
    "# Moreover, we can create our own stop word list and use it to clean the text as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Drunkbear\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his', 'themselves', 'theirs', \"shouldn't\", 'mustn', 'under', 'aren', 'ours', \"mustn't\", \"should've\", 'at', 'did', 'as', 'from', 'yours', 'who', 'when', 'a', 'she', 'by', 'more', 'won', 'again', 'once', 'all', 'very', 'will', 'hers', 'on', 'you', 'are', 'have', 'hadn', 'during', 'between', 'wouldn', 'wasn', 'after', 'both', 'where', 'your', \"don't\", 'until', 'too', 're', 'y', 'haven', 'here', 'don', 'such', 'd', 'having', \"that'll\", \"hasn't\", \"weren't\", 'while', 'him', 'because', 'he', 'am', 'is', 'be', 'i', 'which', 'didn', 'the', 'down', 'out', \"aren't\", \"needn't\", 'had', 'same', \"couldn't\", 'o', 'ma', 'm', 'shouldn', 'to', 'most', \"won't\", 'over', 's', 'were', 'no', 'should', 'can', \"hadn't\", 'my', 'that', 'further', 'was', 'each', 'me', 'these', 'ain', 'but', 'being', 'has', 'nor', 'with', \"mightn't\", 'than', 'how', 'myself', 'shan', 'through', 'some', 'before', \"wasn't\", 'off', 'any', 'himself', 'it', 'them', 'couldn', 'above', 'hasn', 'we', 'there', \"you've\", 'been', 'doesn', 'then', 'if', \"you'd\", 'itself', 'whom', 've', \"doesn't\", 'or', 'other', \"it's\", 'its', 'so', 'they', 'herself', \"isn't\", \"wouldn't\", \"she's\", 'an', 'only', 'not', \"you're\", 'up', 'into', 'in', 'few', 'does', 'doing', 'ourselves', \"you'll\", 'below', 'just', 'mightn', 'about', 'those', 'yourself', 'isn', 'do', 'yourselves', 'why', 'weren', 'for', 'own', 'll', 'our', 'her', 'what', 't', 'and', 'their', 'this', 'now', \"didn't\", 'needn', \"shan't\", 'of', 'against', \"haven't\"} \n",
      "\n",
      "They look good and stick good ! I just do n't like the round shape because I be always bump it and Siri keep pop up and it be irritating. I just wo n't buy a product like this again \n",
      "\n",
      "['look', 'good', 'stick', 'good', '!', \"n't\", 'like', 'round', 'shape', 'always', 'bump', 'siri', 'keep', 'pop', 'irritating.', 'wo', \"n't\", 'buy', 'product', 'like']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StopWord Removal\n",
    "# In sentences there are often words that dont provide any additional information\n",
    "# These are a,is,the,etc\n",
    "# Before any text analysis, these have to be removed as well\n",
    "\n",
    "stop = set(stopwords.words('english'))# This is a set\n",
    "print(stop, '\\n')\n",
    "# Lemmatization\n",
    "sentnc=\" \".join(lemma.lemmatize(i,'v') for i in tokens)\n",
    "print(sentnc, '\\n')\n",
    "# Removing Stop word\n",
    "clen_sentnc=[x for x in sentnc.lower().split() if x not in stop]\n",
    "print(clen_sentnc)\n",
    "len(clen_sentnc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wo'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom stop word list can also be created to remove additional words\n",
    "# For isntance, lets say word 'wo' has to be removed from clen_sentnc\n",
    "# Updating the 'stop' list\n",
    "stop_new=list(stop)\n",
    "stop_new.append('wo')\n",
    "\n",
    "#Checking if 'get' has been added to the new stop list\n",
    "stop_new[len(stop_new)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['look', 'good', 'stick', 'good', '!', \"n't\", 'like', 'round', 'shape', 'always', 'bump', 'siri', 'keep', 'pop', 'irritating.', \"n't\", 'buy', 'product', 'like']\n"
     ]
    }
   ],
   "source": [
    "# Removing 'wo' from the list\n",
    "ls_new = [x for x in clen_sentnc if x not in stop_new ]\n",
    "print(ls_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words using Term Frequency (TF and ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n't            2\n",
      "like           2\n",
      "good           2\n",
      "bump           1\n",
      "always         1\n",
      "round          1\n",
      "irritating.    1\n",
      "look           1\n",
      "keep           1\n",
      "shape          1\n",
      "stick          1\n",
      "pop            1\n",
      "buy            1\n",
      "!              1\n",
      "product        1\n",
      "siri           1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the number of times a word appears in a list\n",
    "# For calculating the word frequency we will use ngrams,FreqDist methods from nltk\n",
    "from nltk import ngrams, FreqDist\n",
    "\n",
    "s1 = pd.Series(ls_new)\n",
    "print(s1.value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (to be continued................)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
